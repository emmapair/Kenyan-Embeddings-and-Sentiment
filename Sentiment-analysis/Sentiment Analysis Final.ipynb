{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/mhagiwara/realworldnlp.git\n",
    "%cd nikit\n",
    "%cd Allen NLP\n",
    "%cd realworldnlp\n",
    "from realworldnlp.predictors import SentenceClassifierPredictor\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data import DataLoader, TextFieldTensors\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import GradientDescentTrainer as Trainer\n",
    "from allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "    StanfordSentimentTreeBankDatasetReader\n",
    "from typing import Dict\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.classification\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = StanfordSentimentTreeBankDatasetReader()\n",
    "\n",
    "train_dataset = reader.read('data/stanfordSentimentTreebank/trees/train.txt')\n",
    "dev_dataset = reader.read('data/stanfordSentimentTreebank/trees/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
    "                                  min_count={'tokens': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary,\n",
    "                 positive_label: str = '4') -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        positive_index = vocab.get_token_index(positive_label, namespace='labels')\n",
    "        self.f1_measure = F1Measure(positive_index)\n",
    "\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.hidden2tag(encoder_out)\n",
    "\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            self.f1_measure(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        precision, recall, f1_measure = self.f1_measure.get_metric(reset)\n",
    "        return {'accuracy': self.accuracy.get_metric(reset),\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_measure': f1_measure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = LstmClassifier(word_embeddings, lstm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAINING\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                                   batch_sampler=BucketBatchSampler(\n",
    "                                       train_dataset,\n",
    "                                       batch_size=32,\n",
    "                                       sorting_keys=[\"tokens\"]))\n",
    "dev_data_loader = DataLoader(dev_dataset,\n",
    "                                 batch_sampler=BucketBatchSampler(\n",
    "                                     dev_dataset,\n",
    "                                     batch_size=32,\n",
    "                                     sorting_keys=[\"tokens\"]))\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  data_loader = train_data_loader,\n",
    "                  validation_data_loader=dev_data_loader,\n",
    "                  patience=10,\n",
    "                  num_epochs=20)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TESTING\n",
    "\n",
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict('This is a happy movie!')['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##INPUT TEXT\n",
    "filename = r\"Sentences\\daily_nation_oped_lifestyle_news_business_counties_sports_1998_2000_sentences_female1.txt\"\n",
    "Df_pd = pd.read_csv(filename,encoding = 'utf-8', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Df_pd.transpose()\n",
    "text['Score'] = 100\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PREDICT\n",
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "\n",
    "for ind in text.index:\n",
    "    inputText = text[0][ind]\n",
    "    logits = predictor.predict(inputText)['logits']\n",
    "    label_id = np.argmax(logits) \n",
    "    a= model.vocab.get_token_from_index(label_id, 'labels')\n",
    "    text.at[ind, 'Score']=a\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_csv(r'Predictions\\98female1.csv')\n",
    "my_tab = text.Score.value_counts()\n",
    "my_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CALCULATING % SENTIMENTS FOR ALL YEARS - AFTER ALL YEARS' CSV SAVED\n",
    "female98 = pd.read_csv(r\"Predictions\\98female1.csv\",encoding = 'utf-8')\n",
    "female98['Year'] = 1998 \n",
    "female01 = pd.read_csv(r\"Predictions\\01female1.csv\",encoding = 'utf-8')\n",
    "female01['Year'] = 2001\n",
    "female04 = pd.read_csv(r\"Predictions\\04female1.csv\",encoding = 'utf-8')\n",
    "female04['Year'] = 2004\n",
    "female07 = pd.read_csv(r\"Predictions\\07female1.csv\",encoding = 'utf-8')\n",
    "female07['Year'] = 2007 \n",
    "female10 = pd.read_csv(r\"Predictions\\10female1.csv\",encoding = 'utf-8')\n",
    "female10['Year'] = 2010 \n",
    "female13 = pd.read_csv(r\"Predictions\\13female1.csv\",encoding = 'utf-8')\n",
    "female13['Year'] = 2013 \n",
    "female16 = pd.read_csv(r\"Predictions\\16female1.csv\",encoding = 'utf-8')\n",
    "female16['Year'] = 2016 \n",
    "\n",
    "female = female98.append([female01, female04, female07, female10, female13, female16])\n",
    "\n",
    "my_tab= pd.crosstab(index=female[\"Year\"], columns = female['Score'])\n",
    "my_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male98 = pd.read_csv(r\"Predictions\\98male1.csv\",encoding = 'utf-8')\n",
    "male98['Year'] = 1998 \n",
    "male01 = pd.read_csv(r\"Predictions\\01male1.csv\",encoding = 'utf-8')\n",
    "male01['Year'] = 2001\n",
    "male04 = pd.read_csv(r\"Predictions\\04male1.csv\",encoding = 'utf-8')\n",
    "male04['Year'] = 2004\n",
    "male07 = pd.read_csv(r\"Predictions\\07male1.csv\",encoding = 'utf-8')\n",
    "male07['Year'] = 2007 \n",
    "male10 = pd.read_csv(r\"Predictions\\10male1.csv\",encoding = 'utf-8')\n",
    "male10['Year'] = 2010 \n",
    "male13 = pd.read_csv(r\"Predictions\\13male1.csv\",encoding = 'utf-8')\n",
    "male13['Year'] = 2013 \n",
    "male16 = pd.read_csv(r\"Predictions\\16male1.csv\",encoding = 'utf-8')\n",
    "male16['Year'] = 2016 \n",
    "\n",
    "male = male98.append([male01, male04, male07, male10, male13, male16])\n",
    "\n",
    "my_tab= pd.crosstab(index=male[\"Year\"], columns = male['Score'])\n",
    "my_tab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
